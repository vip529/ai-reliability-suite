# ğŸ§© AI Reliability Suite

A full-stack, open-source suite of tools for evaluating, stress-testing, and improving the reliability of large language models (LLMs) and agentic AI systems.

Built using **Next.js (App Router)**, **Vercel AI SDK**, and **Supabase**, the suite demonstrates
cutting-edge practices in AI tooling, structured evaluation, and developer-focused UX â€” all designed for Edge-first environments.

---

## ğŸ§  Projects Overview

| Project | Description | Stack |
|----------|--------------|--------|
| ğŸ§© [EvalForge](./apps/evalforge) | Structured LLM evaluation framework supporting rubric-based, schema-validated, and metamorphic testing. | Next.js â€¢ Vercel AI SDK â€¢ Supabase â€¢ Zod |
| âš™ï¸ [SynthBench](./apps/synthbench) | Synthetic data and edge-case generator with automated validation and integration into EvalForge. | Next.js â€¢ Vercel AI SDK â€¢ Supabase â€¢ OpenAI API |
| ğŸ” [AgentReliabilityLab](./apps/agentreliabilitylab) | Agent reliability sandbox featuring tool-calling, schema contracts, retry loops, and trace visualization. | Next.js â€¢ Vercel AI SDK â€¢ Supabase â€¢ React Flow |

Each module can run independently but together they form a complete workflow for
**testing, evaluating, and debugging LLM-based systems**.

---

## ğŸ—ï¸ Architecture
ai-reliability-suite/
â”‚
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ evalforge/              # Structured evaluation system
â”‚   â”œâ”€â”€ synthbench/             # Synthetic data generator
â”‚   â””â”€â”€ agentreliabilitylab/    # Agent reliability dashboard
â”‚
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ core-eval/              # Rubric & schema scoring logic
â”‚   â”œâ”€â”€ core-data/              # DSL fuzzers & validation utilities
â”‚   â”œâ”€â”€ core-agent/             # Planner, executor, and tool orchestration
â”‚   â”œâ”€â”€ db/                     # Prisma + Supabase schema
â”‚   â””â”€â”€ ui/                     # Shared React & styling components
â”‚
â””â”€â”€ deployment/
â”œâ”€â”€ vercel.json             # Vercel config for monorepo deploy
â”œâ”€â”€ .env.example            # Environment variables
â””â”€â”€ seed/                   # Demo datasets & tasks

---

## ğŸš€ Tech Stack

**Core:**  
- Next.js (App Router)  
- Vercel AI SDK  
- TypeScript + Zod  
- Supabase (Postgres + Auth)  
- Prisma (ORM)  

**Supporting:**  
- React Flow (visualization)  
- Recharts / Chart.js (metrics)  
- Tailwind + shadcn/ui (UI system)  
- Better-Auth or NextAuth (optional authentication)  
- Edge Functions for fast AI route execution  

---

## âš¡ Key Highlights
- ğŸ§± **Full-stack AI tooling** â€” end-to-end architecture with reusable core packages  
- âš™ï¸ **Edge-native execution** via Vercel AI SDK and serverless routes  
- ğŸ“Š **Structured evaluation** with schema and rubric validation  
- ğŸ” **Agent reliability framework** for multi-step tool-using systems  
- ğŸ§© **Modular monorepo** built for composability, scalability, and open contribution  

---

## ğŸŒ Live Deployments (Planned)
| App | URL | Status |
|------|-----|---------|
| EvalForge | [evalforge.vercel.app](#) | ğŸš§ In Development |
| SynthBench | [synthbench.vercel.app](#) | ğŸš§ In Development |
| AgentReliabilityLab | [agentreliabilitylab.vercel.app](#) | ğŸš§ In Development |

---

## ğŸ“¦ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/vip529/ai-reliability-suite.git
cd ai-reliability-suite
```

### 2ï¸âƒ£ Install Dependencies
```bash
pnpm install
```

3ï¸âƒ£ Set Up Environment Variables

Copy .env.example â†’ .env.local
Add your keys:

OPENAI_API_KEY=...
SUPABASE_URL=...
SUPABASE_ANON_KEY=...
DATABASE_URL=...

4ï¸âƒ£ Run in Development

pnpm dev

Each app can be started independently:

pnpm --filter evalforge dev

ğŸ§© Roadmap
	â€¢	EvalForge MVP â€” Rubric & schema evaluation
	â€¢	SynthBench MVP â€” Synthetic data generator
	â€¢	AgentReliabilityLab MVP â€” Tool orchestration sandbox
	â€¢	Supabase integration for unified storage
	â€¢	Auth + user profiles
	â€¢	Public dashboards & reports

â¸»

ğŸ§  Vision

AI systems are only as reliable as their evaluation and observability frameworks.
The AI Reliability Suite aims to provide a modular, open-source foundation for
developers and researchers to:
	â€¢	Evaluate LLMs under structured, consistent criteria
	â€¢	Generate adversarial and edge-case datasets automatically
	â€¢	Monitor and visualize agentic reasoning reliability

All while staying Edge-first, type-safe, and developer-friendly.

â¸»

ğŸ‘¨â€ğŸ’» Author

Vipin Yadav
Senior Software Engineer â€” Full Stack & AI Product Developer
LinkedIn â€¢ GitHub
â¸»
