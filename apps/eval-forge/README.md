# ðŸ§  EvalForge â€” Structured LLM Evaluation Framework

EvalForge is a full-stack framework for evaluating large-language-model (LLM) outputs using
structured rubrics, schema validation, and metamorphic testing.

Built with **Next.js**, **Vercel AI SDK**, and **Supabase**, it lets developers define evaluation
tasks (prompt + rubric + expected schema) and automatically compute rubric-weighted scores and
explanations for model responses.

## âœ¨ Features
- ðŸ”¹ Rubric-based and metamorphic evaluation of model outputs  
- ðŸ”¹ JSON Schema & Zod-based structured output validation  
- ðŸ”¹ Edge-deployed routes via **Vercel AI SDK**  
- ðŸ”¹ **Supabase Postgres** for storing tasks, runs, and reports  
- ðŸ”¹ Markdown & JSON report generation for reproducible scoring

## ðŸ§± Tech Stack
Next.js (App Router) â€¢ Vercel AI SDK â€¢ TypeScript â€¢ Supabase â€¢ Zod â€¢ Edge Functions

## ðŸ§© Why It Matters
EvalForge provides a foundation for consistent, reproducible LLM benchmarking and reliability
trackingâ€”critical for evaluating agentic systems and structured outputs.